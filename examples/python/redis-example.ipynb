{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import time \n",
    "import h5py\n",
    "\n",
    "import cyborg_vector_search_py as cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset = \"sift-128-euclidean\"\n",
    "dimension = 128\n",
    "index_type = \"IVFFlat\"\n",
    "n_lists = 4096\n",
    "metric = \"euclidean\"\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 2048\n",
    "max_iters = 100\n",
    "max_dataset_size = 1000000\n",
    "training_ratio = 0.1 # Number of vectors to use for training (rest will be indexed post-training)\n",
    "top_k = 100\n",
    "\n",
    "# Location configuration (Redis)\n",
    "index_location = cvs.DBConfig(location='redis', table_name=\"index_table\", connection_string=\"host:127.0.0.1,port:6379,db:0\")\n",
    "config_location = cvs.DBConfig(location='redis', table_name=\"config_table\", connection_string=\"host:127.0.0.1,port:6379,db:0\")\n",
    "items_location = cvs.DBConfig(location='redis', table_name=\"items_table\", connection_string=\"host:127.0.0.1,port:6379,db:0\")\n",
    "\n",
    "# Index configuration\n",
    "index_config = cvs.IndexIVFFlat(dimension, n_lists, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set where your dataset is located\n",
    "dataset_location = f'../../../../datasets/{dataset}.hdf5'\n",
    "\n",
    "# Load the dataset\n",
    "with h5py.File(dataset_location, 'r') as file:\n",
    "    train = np.array(file['train'], dtype=np.float32)\n",
    "    test = np.array(file['test'], dtype=np.float32)\n",
    "    neighbors = np.array(file['neighbors'], dtype=np.int32)\n",
    "\n",
    "# Setup the training data\n",
    "vectors = train[:max_dataset_size]\n",
    "ids = np.arange(max_dataset_size)\n",
    "training_size = int(max_dataset_size * training_ratio)\n",
    "training_vectors = vectors[:training_size]\n",
    "training_ids = ids[:training_size]\n",
    "remaining_vectors = vectors[training_size:]\n",
    "remaining_ids = ids[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the CVS index\n",
    "client = cvs.Client(\n",
    "    index_location=index_location,\n",
    "    config_location=config_location,\n",
    "    cpu_threads = 16,\n",
    "    gpu_accelerate = False\n",
    ")\n",
    "\n",
    "# Dummy index name and key\n",
    "index_name = \"memory_example_index\"\n",
    "index_key = bytes([1] * 32)\n",
    "\n",
    "# Create the index\n",
    "index = client.create_index(index_name, index_key, index_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert untrained\n",
    "\n",
    "start = time.time()\n",
    "index.upsert(training_ids, training_vectors)\n",
    "print(f\"Upserted {training_size} vectors in {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untrained query\n",
    "\n",
    "n_probes = 10\n",
    "num_queries = 1000\n",
    "initial_queries = test[:num_queries]\n",
    "\n",
    "start = time.time()\n",
    "results = index.query(initial_queries, top_k, n_probes)\n",
    "end = time.time()\n",
    "\n",
    "# Compute the neighbors for the training_size vectors\n",
    "dists = np.sum(training_vectors**2, axis=1) - 2 * np.dot(initial_queries, training_vectors.T) + np.sum(initial_queries**2, axis=1)[:, np.newaxis]\n",
    "initial_neighbors = np.argpartition(dists, top_k, axis=1)[:, :top_k]\n",
    "initial_neighbors = np.take_along_axis(initial_neighbors, np.argsort(dists[np.arange(dists.shape[0])[:, None], initial_neighbors], axis=1), axis=1)\n",
    "\n",
    "result_ids = [\n",
    "    [res[\"id\"] for res in query_results] for query_results in results\n",
    "]\n",
    "\n",
    "# Compute the recall using the neighbors\n",
    "recall = np.zeros(initial_queries.shape[0])\n",
    "for i in range(initial_queries.shape[0]):\n",
    "    recall[i] = len(np.intersect1d(initial_neighbors[i], result_ids[i])) / len(initial_neighbors[i])\n",
    "\n",
    "print(f\"Queried {initial_queries.shape[0]} vectors in {end - start:.2f} seconds\")\n",
    "print(f\"QPS: {num_queries / (end - start):.2f}\")\n",
    "print(f\"Mean recall: {recall.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train index for faster queries\n",
    "\n",
    "start = time.time()\n",
    "index.train(batch_size, max_iters)\n",
    "print(f\"Trained index with {training_size} vectors in {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained query\n",
    "\n",
    "n_probes = 32\n",
    "\n",
    "# Query the test set\n",
    "start = time.time()\n",
    "results = index.query(initial_queries, top_k, n_probes)\n",
    "end = time.time()\n",
    "\n",
    "result_ids = [\n",
    "    [res[\"id\"] for res in query_results] for query_results in results\n",
    "]\n",
    "\n",
    "# Compute the recall using the neighbors\n",
    "result_ids = np.array(result_ids)\n",
    "recall = np.zeros(initial_queries.shape[0])\n",
    "for i in range(initial_queries.shape[0]):\n",
    "    recall[i] = len(np.intersect1d(initial_neighbors[i], result_ids[i])) / len(initial_neighbors[i])\n",
    "\n",
    "print(f\"Queried {initial_queries.shape[0]} vectors in {end - start:.2f} seconds\")\n",
    "print(f\"QPS: {num_queries / (end - start):.2f}\")\n",
    "print(f\"Mean recall: {recall.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert remaining (skip if training ratio is 1)\n",
    "if training_ratio < 1:\n",
    "\n",
    "    start = time.time()\n",
    "    index.upsert(remaining_ids, remaining_vectors)\n",
    "    print(f\"Upserted {remaining_vectors.shape[0]} vectors in {time.time() - start:.2f} seconds\")\n",
    "    print(f\"VPS: {remaining_vectors.shape[0] / (time.time() - start):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained query\n",
    "\n",
    "n_probes = 32\n",
    "\n",
    "# Query the test set\n",
    "start = time.time()\n",
    "results = index.query(test, top_k, n_probes)\n",
    "end = time.time()\n",
    "\n",
    "result_ids = [\n",
    "    [res[\"id\"] for res in query_results] for query_results in results\n",
    "]\n",
    "\n",
    "# Convert the results to numpy array\n",
    "result_ids = np.array(result_ids)\n",
    "\n",
    "# Compute the recall using the neighbors\n",
    "recall = np.zeros(test.shape[0])\n",
    "for i in range(test.shape[0]):\n",
    "    recall[i] = len(np.intersect1d(neighbors[i], result_ids[i])) / len(neighbors[i])\n",
    "\n",
    "print(f\"Queried {test.shape[0]} vectors in {end - start:.2f} seconds\")\n",
    "print(f\"QPS: {test.shape[0] / (end - start):.2f}\")\n",
    "print(f\"Mean recall: {recall.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "\n",
    "index.delete_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
